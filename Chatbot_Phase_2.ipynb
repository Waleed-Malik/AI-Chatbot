{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plwBT7CK6PIr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM , Dense, GlobalMaxPooling1D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "connecting_words = ['and', 'or', 'but', 'for', 'nor', 'so', 'yet', 'after', 'although', 'as', 'because', 'before', 'if', 'once', 'since', 'though', 'unless', 'until', 'when', 'where', 'while', 'that', 'which', 'who', 'whom', 'whose', 'what', 'whose', 'how', 'why','on']\n",
        "articles = ['a', 'an', 'the']"
      ],
      "metadata": {
        "id": "V2cpJPuVPYkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b2HevUk8LPA"
      },
      "outputs": [],
      "source": [
        "with open('data.json') as content:\n",
        "  data1 = json.load(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgSn7mjX98Lx"
      },
      "outputs": [],
      "source": [
        "tags = []\n",
        "inputs = []\n",
        "responses={}\n",
        "for intent in data1['intents']:\n",
        "  responses[intent['tag']] = intent['responses']\n",
        "  for lines in intent['inputs']:\n",
        "    inputs.append(lines)\n",
        "    tags.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5ce3jthAWvI"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame({\"inputs\": inputs, \"tags\" : tags})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "901Y4ObEAjL_",
        "outputId": "1b5fd307-6a36-43c3-cb61-c101828aa8fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       inputs                    tags\n",
              "0                          Hi                greeting\n",
              "1                How are you?                greeting\n",
              "2            Is anyone there?                greeting\n",
              "3                       Hello                greeting\n",
              "4                        Heya                greeting\n",
              "..                        ...                     ...\n",
              "290          how to unfollow.  unfollow_someone_insta\n",
              "291    process of unfollowing  unfollow_someone_insta\n",
              "292                  unfollow  unfollow_someone_insta\n",
              "293          unfollow someone  unfollow_someone_insta\n",
              "294  want to unfollow someone  unfollow_someone_insta\n",
              "\n",
              "[295 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-267812b3-b0be-42b8-a0c2-441e117dcf48\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How are you?</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Is anyone there?</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Heya</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>how to unfollow.</td>\n",
              "      <td>unfollow_someone_insta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>process of unfollowing</td>\n",
              "      <td>unfollow_someone_insta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>unfollow</td>\n",
              "      <td>unfollow_someone_insta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>unfollow someone</td>\n",
              "      <td>unfollow_someone_insta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>want to unfollow someone</td>\n",
              "      <td>unfollow_someone_insta</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>295 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-267812b3-b0be-42b8-a0c2-441e117dcf48')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-267812b3-b0be-42b8-a0c2-441e117dcf48 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-267812b3-b0be-42b8-a0c2-441e117dcf48');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ad17c352-3849-4fc3-8f49-2296308253d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ad17c352-3849-4fc3-8f49-2296308253d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ad17c352-3849-4fc3-8f49-2296308253d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 295,\n  \"fields\": [\n    {\n      \"column\": \"inputs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 288,\n        \"samples\": [\n          \"your owners\",\n          \"how to like a post\",\n          \"frfr\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"follow_someone_on_insta\",\n          \"comment\",\n          \"add_post\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhX4N8LtAswN",
        "outputId": "644c452b-b078-4f05-b181-4554837ff77f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                            hi\n",
              "1                       are you\n",
              "2               is anyone there\n",
              "3                         hello\n",
              "4                          heya\n",
              "                 ...           \n",
              "290                 to unfollow\n",
              "291      process of unfollowing\n",
              "292                    unfollow\n",
              "293            unfollow someone\n",
              "294    want to unfollow someone\n",
              "Name: inputs, Length: 295, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import string\n",
        "data['inputs'] = data['inputs'].apply(lambda wrd: [ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
        "data['inputs'] = data['inputs'].apply(lambda wrd:''. join(wrd))\n",
        "\n",
        "def replace_connecting_words(sentence):\n",
        "    words = sentence.split()\n",
        "    words = [word for word in words if word.lower() not in connecting_words and word.lower() not in articles]\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['inputs'] = data['inputs'].apply(replace_connecting_words)\n",
        "data['inputs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QnKL598BaMh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(data['inputs'])\n",
        "train = tokenizer.texts_to_sequences(data['inputs'])\n",
        "\n",
        "from tensorflow. keras.preprocessing.sequence import pad_sequences\n",
        "x_train = pad_sequences(train)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(data['tags'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AkWxmS0GtTAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVByLMj3DAc3",
        "outputId": "84195ccd-df76-479c-bcd9-f7ac9264fa1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "input_shape = x_train.shape[1]\n",
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsCPGxGGDI56",
        "outputId": "5daa85c0-dabc-4a97-cf63-e2cbac289d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of unique words:  204\n",
            "output length:  29\n"
          ]
        }
      ],
      "source": [
        "vocabulary = len(tokenizer.word_index)\n",
        "print(\"number of unique words: \", vocabulary)\n",
        "\n",
        "output_length = le.classes_.shape[0]\n",
        "print(\"output length: \", output_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD5RP9f5Dndz"
      },
      "outputs": [],
      "source": [
        "i = Input(shape=(input_shape,))\n",
        "x = Embedding(vocabulary + 1, 10)(i)\n",
        "x = LSTM(10, return_sequences=True)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(output_length, activation=\"softmax\")(x)\n",
        "model = Model(i, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIt-OvLLDrJb"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E13JuVIZD0Qr",
        "outputId": "150a3b90-ac1d-43a5-8bb5-e8cb5d9b3e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/350\n",
            "8/8 [==============================] - 4s 103ms/step - loss: 3.3674 - accuracy: 0.0297 - val_loss: 3.3666 - val_accuracy: 0.0678\n",
            "Epoch 2/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.3641 - accuracy: 0.0508 - val_loss: 3.3655 - val_accuracy: 0.0508\n",
            "Epoch 3/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.3613 - accuracy: 0.0805 - val_loss: 3.3644 - val_accuracy: 0.0847\n",
            "Epoch 4/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 3.3583 - accuracy: 0.1271 - val_loss: 3.3629 - val_accuracy: 0.1356\n",
            "Epoch 5/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.3548 - accuracy: 0.1356 - val_loss: 3.3611 - val_accuracy: 0.1186\n",
            "Epoch 6/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.3505 - accuracy: 0.1441 - val_loss: 3.3589 - val_accuracy: 0.1186\n",
            "Epoch 7/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.3452 - accuracy: 0.1314 - val_loss: 3.3562 - val_accuracy: 0.1356\n",
            "Epoch 8/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.3391 - accuracy: 0.1525 - val_loss: 3.3532 - val_accuracy: 0.1356\n",
            "Epoch 9/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.3310 - accuracy: 0.1525 - val_loss: 3.3502 - val_accuracy: 0.1356\n",
            "Epoch 10/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 3.3221 - accuracy: 0.1271 - val_loss: 3.3462 - val_accuracy: 0.1186\n",
            "Epoch 11/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.3108 - accuracy: 0.1398 - val_loss: 3.3417 - val_accuracy: 0.1186\n",
            "Epoch 12/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 3.2974 - accuracy: 0.1229 - val_loss: 3.3385 - val_accuracy: 0.1017\n",
            "Epoch 13/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.2832 - accuracy: 0.1059 - val_loss: 3.3354 - val_accuracy: 0.1017\n",
            "Epoch 14/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.2684 - accuracy: 0.0890 - val_loss: 3.3347 - val_accuracy: 0.1017\n",
            "Epoch 15/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 3.2506 - accuracy: 0.0975 - val_loss: 3.3320 - val_accuracy: 0.1017\n",
            "Epoch 16/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.2346 - accuracy: 0.1017 - val_loss: 3.3231 - val_accuracy: 0.0847\n",
            "Epoch 17/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.2137 - accuracy: 0.1017 - val_loss: 3.3093 - val_accuracy: 0.0847\n",
            "Epoch 18/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.1931 - accuracy: 0.0847 - val_loss: 3.2977 - val_accuracy: 0.0847\n",
            "Epoch 19/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 3.1685 - accuracy: 0.0763 - val_loss: 3.2791 - val_accuracy: 0.0847\n",
            "Epoch 20/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3.1420 - accuracy: 0.0763 - val_loss: 3.2621 - val_accuracy: 0.0847\n",
            "Epoch 21/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 3.1117 - accuracy: 0.0805 - val_loss: 3.2500 - val_accuracy: 0.1017\n",
            "Epoch 22/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 3.0773 - accuracy: 0.0932 - val_loss: 3.2347 - val_accuracy: 0.1186\n",
            "Epoch 23/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 3.0410 - accuracy: 0.1229 - val_loss: 3.2129 - val_accuracy: 0.1356\n",
            "Epoch 24/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 3.0027 - accuracy: 0.1314 - val_loss: 3.1888 - val_accuracy: 0.1356\n",
            "Epoch 25/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 2.9613 - accuracy: 0.1356 - val_loss: 3.1754 - val_accuracy: 0.1356\n",
            "Epoch 26/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.9185 - accuracy: 0.1737 - val_loss: 3.1587 - val_accuracy: 0.1356\n",
            "Epoch 27/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.8749 - accuracy: 0.1864 - val_loss: 3.1311 - val_accuracy: 0.1695\n",
            "Epoch 28/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.8285 - accuracy: 0.2076 - val_loss: 3.1101 - val_accuracy: 0.1864\n",
            "Epoch 29/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 2.7790 - accuracy: 0.2288 - val_loss: 3.0832 - val_accuracy: 0.2034\n",
            "Epoch 30/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.7305 - accuracy: 0.2415 - val_loss: 3.0604 - val_accuracy: 0.1864\n",
            "Epoch 31/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.6777 - accuracy: 0.2373 - val_loss: 3.0411 - val_accuracy: 0.2203\n",
            "Epoch 32/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.6299 - accuracy: 0.2500 - val_loss: 2.9996 - val_accuracy: 0.2203\n",
            "Epoch 33/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.5766 - accuracy: 0.2669 - val_loss: 2.9894 - val_accuracy: 0.2203\n",
            "Epoch 34/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.5236 - accuracy: 0.2881 - val_loss: 2.9385 - val_accuracy: 0.2203\n",
            "Epoch 35/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.4734 - accuracy: 0.3220 - val_loss: 2.9290 - val_accuracy: 0.2373\n",
            "Epoch 36/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.4211 - accuracy: 0.3559 - val_loss: 2.8969 - val_accuracy: 0.2203\n",
            "Epoch 37/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.3792 - accuracy: 0.3347 - val_loss: 2.8776 - val_accuracy: 0.2373\n",
            "Epoch 38/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 2.3240 - accuracy: 0.3898 - val_loss: 2.8667 - val_accuracy: 0.2542\n",
            "Epoch 39/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.2790 - accuracy: 0.4068 - val_loss: 2.8269 - val_accuracy: 0.2203\n",
            "Epoch 40/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 2.2358 - accuracy: 0.4195 - val_loss: 2.8098 - val_accuracy: 0.2373\n",
            "Epoch 41/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.1899 - accuracy: 0.4195 - val_loss: 2.7929 - val_accuracy: 0.2542\n",
            "Epoch 42/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.1477 - accuracy: 0.4788 - val_loss: 2.7624 - val_accuracy: 0.2712\n",
            "Epoch 43/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.1130 - accuracy: 0.4788 - val_loss: 2.7364 - val_accuracy: 0.2712\n",
            "Epoch 44/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.0658 - accuracy: 0.4958 - val_loss: 2.7110 - val_accuracy: 0.2712\n",
            "Epoch 45/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 2.0265 - accuracy: 0.4958 - val_loss: 2.6924 - val_accuracy: 0.2712\n",
            "Epoch 46/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.9904 - accuracy: 0.5212 - val_loss: 2.6782 - val_accuracy: 0.2542\n",
            "Epoch 47/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.9554 - accuracy: 0.5339 - val_loss: 2.6521 - val_accuracy: 0.2542\n",
            "Epoch 48/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.9209 - accuracy: 0.5551 - val_loss: 2.6397 - val_accuracy: 0.3220\n",
            "Epoch 49/350\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.8936 - accuracy: 0.5805 - val_loss: 2.6231 - val_accuracy: 0.2712\n",
            "Epoch 50/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.8578 - accuracy: 0.5678 - val_loss: 2.6034 - val_accuracy: 0.3051\n",
            "Epoch 51/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.8199 - accuracy: 0.6017 - val_loss: 2.5646 - val_accuracy: 0.3051\n",
            "Epoch 52/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.7964 - accuracy: 0.6483 - val_loss: 2.5449 - val_accuracy: 0.3220\n",
            "Epoch 53/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.7578 - accuracy: 0.6483 - val_loss: 2.5407 - val_accuracy: 0.3051\n",
            "Epoch 54/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.7316 - accuracy: 0.6568 - val_loss: 2.5205 - val_accuracy: 0.3390\n",
            "Epoch 55/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.7033 - accuracy: 0.6653 - val_loss: 2.4918 - val_accuracy: 0.3390\n",
            "Epoch 56/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.6769 - accuracy: 0.6356 - val_loss: 2.4759 - val_accuracy: 0.3051\n",
            "Epoch 57/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.6436 - accuracy: 0.6695 - val_loss: 2.4684 - val_accuracy: 0.3390\n",
            "Epoch 58/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.6179 - accuracy: 0.6822 - val_loss: 2.4543 - val_accuracy: 0.3220\n",
            "Epoch 59/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.5978 - accuracy: 0.6822 - val_loss: 2.4315 - val_accuracy: 0.3220\n",
            "Epoch 60/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.5749 - accuracy: 0.6525 - val_loss: 2.4231 - val_accuracy: 0.3051\n",
            "Epoch 61/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.5465 - accuracy: 0.7288 - val_loss: 2.4088 - val_accuracy: 0.3390\n",
            "Epoch 62/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.5205 - accuracy: 0.6907 - val_loss: 2.3918 - val_accuracy: 0.3051\n",
            "Epoch 63/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.4969 - accuracy: 0.7373 - val_loss: 2.3796 - val_accuracy: 0.3051\n",
            "Epoch 64/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.4762 - accuracy: 0.7542 - val_loss: 2.3678 - val_accuracy: 0.3051\n",
            "Epoch 65/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.4492 - accuracy: 0.7288 - val_loss: 2.3566 - val_accuracy: 0.3220\n",
            "Epoch 66/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.4285 - accuracy: 0.7288 - val_loss: 2.3454 - val_accuracy: 0.3559\n",
            "Epoch 67/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.4041 - accuracy: 0.7627 - val_loss: 2.3310 - val_accuracy: 0.3220\n",
            "Epoch 68/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.3803 - accuracy: 0.7415 - val_loss: 2.3193 - val_accuracy: 0.3220\n",
            "Epoch 69/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 1.3580 - accuracy: 0.7500 - val_loss: 2.3015 - val_accuracy: 0.3559\n",
            "Epoch 70/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 1.3385 - accuracy: 0.7542 - val_loss: 2.2953 - val_accuracy: 0.3559\n",
            "Epoch 71/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.3172 - accuracy: 0.7542 - val_loss: 2.2823 - val_accuracy: 0.3559\n",
            "Epoch 72/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.2975 - accuracy: 0.7797 - val_loss: 2.2698 - val_accuracy: 0.3559\n",
            "Epoch 73/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.2773 - accuracy: 0.7797 - val_loss: 2.2629 - val_accuracy: 0.3559\n",
            "Epoch 74/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.2646 - accuracy: 0.7881 - val_loss: 2.2426 - val_accuracy: 0.3559\n",
            "Epoch 75/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.2488 - accuracy: 0.7754 - val_loss: 2.2452 - val_accuracy: 0.3559\n",
            "Epoch 76/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.2313 - accuracy: 0.8051 - val_loss: 2.2335 - val_accuracy: 0.3220\n",
            "Epoch 77/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.2048 - accuracy: 0.7797 - val_loss: 2.2218 - val_accuracy: 0.3729\n",
            "Epoch 78/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.1879 - accuracy: 0.8136 - val_loss: 2.2124 - val_accuracy: 0.3390\n",
            "Epoch 79/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.1642 - accuracy: 0.7966 - val_loss: 2.2024 - val_accuracy: 0.3729\n",
            "Epoch 80/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.1469 - accuracy: 0.8093 - val_loss: 2.1983 - val_accuracy: 0.3729\n",
            "Epoch 81/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.1317 - accuracy: 0.7924 - val_loss: 2.1936 - val_accuracy: 0.3898\n",
            "Epoch 82/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.1140 - accuracy: 0.8220 - val_loss: 2.1785 - val_accuracy: 0.3898\n",
            "Epoch 83/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.0945 - accuracy: 0.8136 - val_loss: 2.1707 - val_accuracy: 0.3898\n",
            "Epoch 84/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.0773 - accuracy: 0.8178 - val_loss: 2.1634 - val_accuracy: 0.3898\n",
            "Epoch 85/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1.0610 - accuracy: 0.8178 - val_loss: 2.1548 - val_accuracy: 0.4068\n",
            "Epoch 86/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 1.0487 - accuracy: 0.8093 - val_loss: 2.1504 - val_accuracy: 0.4237\n",
            "Epoch 87/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 1.0304 - accuracy: 0.8263 - val_loss: 2.1345 - val_accuracy: 0.3729\n",
            "Epoch 88/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 1.0162 - accuracy: 0.8178 - val_loss: 2.1350 - val_accuracy: 0.4237\n",
            "Epoch 89/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 1.0040 - accuracy: 0.8390 - val_loss: 2.1263 - val_accuracy: 0.4237\n",
            "Epoch 90/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9908 - accuracy: 0.8136 - val_loss: 2.1302 - val_accuracy: 0.4237\n",
            "Epoch 91/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9683 - accuracy: 0.8390 - val_loss: 2.1152 - val_accuracy: 0.4237\n",
            "Epoch 92/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9549 - accuracy: 0.8347 - val_loss: 2.1086 - val_accuracy: 0.4407\n",
            "Epoch 93/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9412 - accuracy: 0.8305 - val_loss: 2.1115 - val_accuracy: 0.4407\n",
            "Epoch 94/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9272 - accuracy: 0.8686 - val_loss: 2.0957 - val_accuracy: 0.4407\n",
            "Epoch 95/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9120 - accuracy: 0.8475 - val_loss: 2.0939 - val_accuracy: 0.4407\n",
            "Epoch 96/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8954 - accuracy: 0.8517 - val_loss: 2.0907 - val_accuracy: 0.4407\n",
            "Epoch 97/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8814 - accuracy: 0.8517 - val_loss: 2.0836 - val_accuracy: 0.4407\n",
            "Epoch 98/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8709 - accuracy: 0.8517 - val_loss: 2.0756 - val_accuracy: 0.4407\n",
            "Epoch 99/350\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.8577 - accuracy: 0.8729 - val_loss: 2.0661 - val_accuracy: 0.4407\n",
            "Epoch 100/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.8438 - accuracy: 0.8602 - val_loss: 2.0669 - val_accuracy: 0.4576\n",
            "Epoch 101/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.8312 - accuracy: 0.8729 - val_loss: 2.0711 - val_accuracy: 0.4746\n",
            "Epoch 102/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.8185 - accuracy: 0.8814 - val_loss: 2.0659 - val_accuracy: 0.4746\n",
            "Epoch 103/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.8075 - accuracy: 0.8898 - val_loss: 2.0569 - val_accuracy: 0.4407\n",
            "Epoch 104/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8011 - accuracy: 0.8602 - val_loss: 2.0635 - val_accuracy: 0.4407\n",
            "Epoch 105/350\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.7958 - accuracy: 0.8898 - val_loss: 2.0508 - val_accuracy: 0.4407\n",
            "Epoch 106/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7724 - accuracy: 0.8941 - val_loss: 2.0556 - val_accuracy: 0.4576\n",
            "Epoch 107/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7595 - accuracy: 0.8898 - val_loss: 2.0515 - val_accuracy: 0.4407\n",
            "Epoch 108/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.7491 - accuracy: 0.8983 - val_loss: 2.0505 - val_accuracy: 0.4407\n",
            "Epoch 109/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7381 - accuracy: 0.9068 - val_loss: 2.0440 - val_accuracy: 0.4407\n",
            "Epoch 110/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.7267 - accuracy: 0.9068 - val_loss: 2.0289 - val_accuracy: 0.4407\n",
            "Epoch 111/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.7174 - accuracy: 0.8856 - val_loss: 2.0362 - val_accuracy: 0.4407\n",
            "Epoch 112/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7070 - accuracy: 0.8941 - val_loss: 2.0298 - val_accuracy: 0.4407\n",
            "Epoch 113/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.6977 - accuracy: 0.8941 - val_loss: 2.0346 - val_accuracy: 0.4407\n",
            "Epoch 114/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6858 - accuracy: 0.8983 - val_loss: 2.0422 - val_accuracy: 0.4407\n",
            "Epoch 115/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.6768 - accuracy: 0.8983 - val_loss: 2.0340 - val_accuracy: 0.4407\n",
            "Epoch 116/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6670 - accuracy: 0.9068 - val_loss: 2.0352 - val_accuracy: 0.4746\n",
            "Epoch 117/350\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6573 - accuracy: 0.9110 - val_loss: 2.0257 - val_accuracy: 0.4746\n",
            "Epoch 118/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.6508 - accuracy: 0.9237 - val_loss: 2.0329 - val_accuracy: 0.4407\n",
            "Epoch 119/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6484 - accuracy: 0.9068 - val_loss: 2.0262 - val_accuracy: 0.4576\n",
            "Epoch 120/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6341 - accuracy: 0.9110 - val_loss: 2.0155 - val_accuracy: 0.4576\n",
            "Epoch 121/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.6222 - accuracy: 0.9237 - val_loss: 2.0178 - val_accuracy: 0.4576\n",
            "Epoch 122/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6131 - accuracy: 0.9195 - val_loss: 2.0180 - val_accuracy: 0.4576\n",
            "Epoch 123/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6040 - accuracy: 0.9237 - val_loss: 2.0118 - val_accuracy: 0.4746\n",
            "Epoch 124/350\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5961 - accuracy: 0.9364 - val_loss: 2.0064 - val_accuracy: 0.4576\n",
            "Epoch 125/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.5866 - accuracy: 0.9322 - val_loss: 2.0039 - val_accuracy: 0.4576\n",
            "Epoch 126/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.5791 - accuracy: 0.9322 - val_loss: 2.0022 - val_accuracy: 0.4576\n",
            "Epoch 127/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.5729 - accuracy: 0.9322 - val_loss: 2.0085 - val_accuracy: 0.4576\n",
            "Epoch 128/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5653 - accuracy: 0.9364 - val_loss: 2.0081 - val_accuracy: 0.4576\n",
            "Epoch 129/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.5569 - accuracy: 0.9407 - val_loss: 1.9961 - val_accuracy: 0.4576\n",
            "Epoch 130/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5503 - accuracy: 0.9407 - val_loss: 2.0004 - val_accuracy: 0.4576\n",
            "Epoch 131/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.5423 - accuracy: 0.9364 - val_loss: 1.9954 - val_accuracy: 0.4576\n",
            "Epoch 132/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5370 - accuracy: 0.9364 - val_loss: 2.0110 - val_accuracy: 0.4576\n",
            "Epoch 133/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.5275 - accuracy: 0.9492 - val_loss: 2.0041 - val_accuracy: 0.4576\n",
            "Epoch 134/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.5216 - accuracy: 0.9449 - val_loss: 2.0063 - val_accuracy: 0.4576\n",
            "Epoch 135/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.5228 - accuracy: 0.9534 - val_loss: 2.0006 - val_accuracy: 0.4746\n",
            "Epoch 136/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5092 - accuracy: 0.9492 - val_loss: 1.9997 - val_accuracy: 0.4746\n",
            "Epoch 137/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.5005 - accuracy: 0.9449 - val_loss: 2.0068 - val_accuracy: 0.4915\n",
            "Epoch 138/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4950 - accuracy: 0.9492 - val_loss: 2.0144 - val_accuracy: 0.4746\n",
            "Epoch 139/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.4919 - accuracy: 0.9534 - val_loss: 2.0020 - val_accuracy: 0.4915\n",
            "Epoch 140/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4870 - accuracy: 0.9407 - val_loss: 2.0046 - val_accuracy: 0.4915\n",
            "Epoch 141/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.4839 - accuracy: 0.9534 - val_loss: 1.9977 - val_accuracy: 0.4915\n",
            "Epoch 142/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4790 - accuracy: 0.9449 - val_loss: 2.0077 - val_accuracy: 0.4915\n",
            "Epoch 143/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4695 - accuracy: 0.9703 - val_loss: 2.0132 - val_accuracy: 0.4915\n",
            "Epoch 144/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4596 - accuracy: 0.9492 - val_loss: 2.0027 - val_accuracy: 0.4915\n",
            "Epoch 145/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4540 - accuracy: 0.9619 - val_loss: 1.9986 - val_accuracy: 0.4746\n",
            "Epoch 146/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4456 - accuracy: 0.9534 - val_loss: 1.9988 - val_accuracy: 0.4915\n",
            "Epoch 147/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4387 - accuracy: 0.9534 - val_loss: 1.9958 - val_accuracy: 0.5085\n",
            "Epoch 148/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4338 - accuracy: 0.9619 - val_loss: 1.9955 - val_accuracy: 0.4915\n",
            "Epoch 149/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4305 - accuracy: 0.9661 - val_loss: 1.9984 - val_accuracy: 0.5085\n",
            "Epoch 150/350\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4251 - accuracy: 0.9534 - val_loss: 2.0059 - val_accuracy: 0.5085\n",
            "Epoch 151/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4183 - accuracy: 0.9661 - val_loss: 2.0129 - val_accuracy: 0.5254\n",
            "Epoch 152/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4132 - accuracy: 0.9661 - val_loss: 2.0106 - val_accuracy: 0.5254\n",
            "Epoch 153/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4081 - accuracy: 0.9703 - val_loss: 2.0066 - val_accuracy: 0.5254\n",
            "Epoch 154/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4057 - accuracy: 0.9576 - val_loss: 2.0018 - val_accuracy: 0.5085\n",
            "Epoch 155/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4010 - accuracy: 0.9619 - val_loss: 2.0028 - val_accuracy: 0.5254\n",
            "Epoch 156/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3921 - accuracy: 0.9703 - val_loss: 2.0096 - val_accuracy: 0.5254\n",
            "Epoch 157/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3875 - accuracy: 0.9746 - val_loss: 2.0046 - val_accuracy: 0.5254\n",
            "Epoch 158/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3825 - accuracy: 0.9746 - val_loss: 2.0038 - val_accuracy: 0.5085\n",
            "Epoch 159/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3773 - accuracy: 0.9831 - val_loss: 2.0032 - val_accuracy: 0.5085\n",
            "Epoch 160/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3752 - accuracy: 0.9746 - val_loss: 1.9936 - val_accuracy: 0.5085\n",
            "Epoch 161/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3707 - accuracy: 0.9788 - val_loss: 2.0071 - val_accuracy: 0.5254\n",
            "Epoch 162/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3651 - accuracy: 0.9788 - val_loss: 2.0050 - val_accuracy: 0.5254\n",
            "Epoch 163/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3602 - accuracy: 0.9788 - val_loss: 2.0072 - val_accuracy: 0.5424\n",
            "Epoch 164/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3557 - accuracy: 0.9831 - val_loss: 2.0064 - val_accuracy: 0.5254\n",
            "Epoch 165/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3519 - accuracy: 0.9788 - val_loss: 2.0095 - val_accuracy: 0.5254\n",
            "Epoch 166/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3497 - accuracy: 0.9788 - val_loss: 2.0120 - val_accuracy: 0.5424\n",
            "Epoch 167/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3459 - accuracy: 0.9788 - val_loss: 2.0101 - val_accuracy: 0.5424\n",
            "Epoch 168/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3440 - accuracy: 0.9788 - val_loss: 2.0154 - val_accuracy: 0.5424\n",
            "Epoch 169/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3383 - accuracy: 0.9831 - val_loss: 2.0113 - val_accuracy: 0.5424\n",
            "Epoch 170/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3319 - accuracy: 0.9788 - val_loss: 2.0087 - val_accuracy: 0.5254\n",
            "Epoch 171/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3299 - accuracy: 0.9788 - val_loss: 2.0221 - val_accuracy: 0.5424\n",
            "Epoch 172/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3277 - accuracy: 0.9788 - val_loss: 2.0156 - val_accuracy: 0.5254\n",
            "Epoch 173/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3200 - accuracy: 0.9788 - val_loss: 2.0157 - val_accuracy: 0.5254\n",
            "Epoch 174/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3177 - accuracy: 0.9831 - val_loss: 2.0211 - val_accuracy: 0.5424\n",
            "Epoch 175/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3134 - accuracy: 0.9788 - val_loss: 2.0158 - val_accuracy: 0.5424\n",
            "Epoch 176/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3110 - accuracy: 0.9788 - val_loss: 2.0187 - val_accuracy: 0.5254\n",
            "Epoch 177/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3073 - accuracy: 0.9788 - val_loss: 2.0155 - val_accuracy: 0.5254\n",
            "Epoch 178/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3031 - accuracy: 0.9788 - val_loss: 2.0202 - val_accuracy: 0.5424\n",
            "Epoch 179/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3003 - accuracy: 0.9788 - val_loss: 2.0150 - val_accuracy: 0.5424\n",
            "Epoch 180/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2969 - accuracy: 0.9788 - val_loss: 2.0183 - val_accuracy: 0.5254\n",
            "Epoch 181/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2935 - accuracy: 0.9831 - val_loss: 2.0198 - val_accuracy: 0.5254\n",
            "Epoch 182/350\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2894 - accuracy: 0.9831 - val_loss: 2.0182 - val_accuracy: 0.5085\n",
            "Epoch 183/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2869 - accuracy: 0.9831 - val_loss: 2.0282 - val_accuracy: 0.5085\n",
            "Epoch 184/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2831 - accuracy: 0.9788 - val_loss: 2.0269 - val_accuracy: 0.5085\n",
            "Epoch 185/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2807 - accuracy: 0.9831 - val_loss: 2.0383 - val_accuracy: 0.5085\n",
            "Epoch 186/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2773 - accuracy: 0.9831 - val_loss: 2.0429 - val_accuracy: 0.5254\n",
            "Epoch 187/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2749 - accuracy: 0.9831 - val_loss: 2.0384 - val_accuracy: 0.5085\n",
            "Epoch 188/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2722 - accuracy: 0.9831 - val_loss: 2.0408 - val_accuracy: 0.5254\n",
            "Epoch 189/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2683 - accuracy: 0.9873 - val_loss: 2.0375 - val_accuracy: 0.5085\n",
            "Epoch 190/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2662 - accuracy: 0.9873 - val_loss: 2.0407 - val_accuracy: 0.5085\n",
            "Epoch 191/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2658 - accuracy: 0.9831 - val_loss: 2.0535 - val_accuracy: 0.5254\n",
            "Epoch 192/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2648 - accuracy: 0.9873 - val_loss: 2.0414 - val_accuracy: 0.5085\n",
            "Epoch 193/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2677 - accuracy: 0.9831 - val_loss: 2.0413 - val_accuracy: 0.5254\n",
            "Epoch 194/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2549 - accuracy: 0.9873 - val_loss: 2.0501 - val_accuracy: 0.5254\n",
            "Epoch 195/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2518 - accuracy: 0.9873 - val_loss: 2.0410 - val_accuracy: 0.5254\n",
            "Epoch 196/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2520 - accuracy: 0.9831 - val_loss: 2.0419 - val_accuracy: 0.5254\n",
            "Epoch 197/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2473 - accuracy: 0.9873 - val_loss: 2.0383 - val_accuracy: 0.5254\n",
            "Epoch 198/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2457 - accuracy: 0.9873 - val_loss: 2.0400 - val_accuracy: 0.5085\n",
            "Epoch 199/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2422 - accuracy: 0.9873 - val_loss: 2.0547 - val_accuracy: 0.5254\n",
            "Epoch 200/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2394 - accuracy: 0.9873 - val_loss: 2.0507 - val_accuracy: 0.5254\n",
            "Epoch 201/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2377 - accuracy: 0.9831 - val_loss: 2.0452 - val_accuracy: 0.5254\n",
            "Epoch 202/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2350 - accuracy: 0.9831 - val_loss: 2.0445 - val_accuracy: 0.5085\n",
            "Epoch 203/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2330 - accuracy: 0.9831 - val_loss: 2.0532 - val_accuracy: 0.5254\n",
            "Epoch 204/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2321 - accuracy: 0.9831 - val_loss: 2.0557 - val_accuracy: 0.5254\n",
            "Epoch 205/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2277 - accuracy: 0.9873 - val_loss: 2.0534 - val_accuracy: 0.5254\n",
            "Epoch 206/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2251 - accuracy: 0.9831 - val_loss: 2.0555 - val_accuracy: 0.5254\n",
            "Epoch 207/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2246 - accuracy: 0.9873 - val_loss: 2.0567 - val_accuracy: 0.5254\n",
            "Epoch 208/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2208 - accuracy: 0.9873 - val_loss: 2.0548 - val_accuracy: 0.5254\n",
            "Epoch 209/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2203 - accuracy: 0.9873 - val_loss: 2.0553 - val_accuracy: 0.5254\n",
            "Epoch 210/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2177 - accuracy: 0.9831 - val_loss: 2.0600 - val_accuracy: 0.5254\n",
            "Epoch 211/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2146 - accuracy: 0.9873 - val_loss: 2.0657 - val_accuracy: 0.5254\n",
            "Epoch 212/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2147 - accuracy: 0.9831 - val_loss: 2.0703 - val_accuracy: 0.5254\n",
            "Epoch 213/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2144 - accuracy: 0.9873 - val_loss: 2.0598 - val_accuracy: 0.5254\n",
            "Epoch 214/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2150 - accuracy: 0.9831 - val_loss: 2.0535 - val_accuracy: 0.5254\n",
            "Epoch 215/350\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2066 - accuracy: 0.9873 - val_loss: 2.0697 - val_accuracy: 0.5254\n",
            "Epoch 216/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2058 - accuracy: 0.9873 - val_loss: 2.0698 - val_accuracy: 0.5254\n",
            "Epoch 217/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2043 - accuracy: 0.9873 - val_loss: 2.0741 - val_accuracy: 0.5254\n",
            "Epoch 218/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2014 - accuracy: 0.9915 - val_loss: 2.0781 - val_accuracy: 0.5254\n",
            "Epoch 219/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1986 - accuracy: 0.9915 - val_loss: 2.0727 - val_accuracy: 0.5254\n",
            "Epoch 220/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.1978 - accuracy: 0.9873 - val_loss: 2.0830 - val_accuracy: 0.5254\n",
            "Epoch 221/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1957 - accuracy: 0.9831 - val_loss: 2.0811 - val_accuracy: 0.5085\n",
            "Epoch 222/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.1938 - accuracy: 0.9831 - val_loss: 2.0795 - val_accuracy: 0.5085\n",
            "Epoch 223/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1929 - accuracy: 0.9831 - val_loss: 2.0789 - val_accuracy: 0.5085\n",
            "Epoch 224/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1900 - accuracy: 0.9873 - val_loss: 2.0778 - val_accuracy: 0.5254\n",
            "Epoch 225/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1899 - accuracy: 0.9831 - val_loss: 2.0905 - val_accuracy: 0.5254\n",
            "Epoch 226/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1902 - accuracy: 0.9873 - val_loss: 2.0777 - val_accuracy: 0.5085\n",
            "Epoch 227/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1894 - accuracy: 0.9831 - val_loss: 2.0751 - val_accuracy: 0.5254\n",
            "Epoch 228/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1836 - accuracy: 0.9831 - val_loss: 2.0881 - val_accuracy: 0.5254\n",
            "Epoch 229/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1822 - accuracy: 0.9873 - val_loss: 2.0815 - val_accuracy: 0.5085\n",
            "Epoch 230/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.1819 - accuracy: 0.9873 - val_loss: 2.0868 - val_accuracy: 0.5085\n",
            "Epoch 231/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1864 - accuracy: 0.9831 - val_loss: 2.0955 - val_accuracy: 0.5085\n",
            "Epoch 232/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.1823 - accuracy: 0.9873 - val_loss: 2.0919 - val_accuracy: 0.5254\n",
            "Epoch 233/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1784 - accuracy: 0.9915 - val_loss: 2.1124 - val_accuracy: 0.5085\n",
            "Epoch 234/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1747 - accuracy: 0.9873 - val_loss: 2.1065 - val_accuracy: 0.5254\n",
            "Epoch 235/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.1784 - accuracy: 0.9915 - val_loss: 2.1037 - val_accuracy: 0.5254\n",
            "Epoch 236/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1761 - accuracy: 0.9915 - val_loss: 2.0958 - val_accuracy: 0.5254\n",
            "Epoch 237/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.1718 - accuracy: 0.9873 - val_loss: 2.1109 - val_accuracy: 0.5254\n",
            "Epoch 238/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1696 - accuracy: 0.9915 - val_loss: 2.0988 - val_accuracy: 0.5254\n",
            "Epoch 239/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1686 - accuracy: 0.9915 - val_loss: 2.0980 - val_accuracy: 0.5254\n",
            "Epoch 240/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1654 - accuracy: 0.9915 - val_loss: 2.1007 - val_accuracy: 0.5254\n",
            "Epoch 241/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1656 - accuracy: 0.9873 - val_loss: 2.0992 - val_accuracy: 0.5254\n",
            "Epoch 242/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1648 - accuracy: 0.9915 - val_loss: 2.1051 - val_accuracy: 0.5254\n",
            "Epoch 243/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1639 - accuracy: 0.9873 - val_loss: 2.1021 - val_accuracy: 0.5254\n",
            "Epoch 244/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1643 - accuracy: 0.9915 - val_loss: 2.1148 - val_accuracy: 0.5254\n",
            "Epoch 245/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1646 - accuracy: 0.9873 - val_loss: 2.0996 - val_accuracy: 0.5424\n",
            "Epoch 246/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1590 - accuracy: 0.9915 - val_loss: 2.1129 - val_accuracy: 0.5254\n",
            "Epoch 247/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1567 - accuracy: 0.9915 - val_loss: 2.1002 - val_accuracy: 0.5424\n",
            "Epoch 248/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1555 - accuracy: 0.9873 - val_loss: 2.1124 - val_accuracy: 0.5424\n",
            "Epoch 249/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1534 - accuracy: 0.9915 - val_loss: 2.1143 - val_accuracy: 0.5424\n",
            "Epoch 250/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1526 - accuracy: 0.9915 - val_loss: 2.1315 - val_accuracy: 0.5254\n",
            "Epoch 251/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1523 - accuracy: 0.9873 - val_loss: 2.1200 - val_accuracy: 0.5424\n",
            "Epoch 252/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1517 - accuracy: 0.9873 - val_loss: 2.1332 - val_accuracy: 0.5254\n",
            "Epoch 253/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1497 - accuracy: 0.9873 - val_loss: 2.1305 - val_accuracy: 0.5254\n",
            "Epoch 254/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1497 - accuracy: 0.9873 - val_loss: 2.1365 - val_accuracy: 0.5254\n",
            "Epoch 255/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1474 - accuracy: 0.9915 - val_loss: 2.1259 - val_accuracy: 0.5424\n",
            "Epoch 256/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1451 - accuracy: 0.9915 - val_loss: 2.1256 - val_accuracy: 0.5254\n",
            "Epoch 257/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1446 - accuracy: 0.9915 - val_loss: 2.1205 - val_accuracy: 0.5424\n",
            "Epoch 258/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1432 - accuracy: 0.9873 - val_loss: 2.1233 - val_accuracy: 0.5424\n",
            "Epoch 259/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1425 - accuracy: 0.9915 - val_loss: 2.1254 - val_accuracy: 0.5424\n",
            "Epoch 260/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1425 - accuracy: 0.9873 - val_loss: 2.1352 - val_accuracy: 0.5424\n",
            "Epoch 261/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1405 - accuracy: 0.9915 - val_loss: 2.1340 - val_accuracy: 0.5424\n",
            "Epoch 262/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1387 - accuracy: 0.9915 - val_loss: 2.1293 - val_accuracy: 0.5424\n",
            "Epoch 263/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1378 - accuracy: 0.9915 - val_loss: 2.1348 - val_accuracy: 0.5424\n",
            "Epoch 264/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1400 - accuracy: 0.9915 - val_loss: 2.1336 - val_accuracy: 0.5424\n",
            "Epoch 265/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1377 - accuracy: 0.9873 - val_loss: 2.1359 - val_accuracy: 0.5424\n",
            "Epoch 266/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.1371 - accuracy: 0.9873 - val_loss: 2.1451 - val_accuracy: 0.5424\n",
            "Epoch 267/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1361 - accuracy: 0.9873 - val_loss: 2.1429 - val_accuracy: 0.5424\n",
            "Epoch 268/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1331 - accuracy: 0.9915 - val_loss: 2.1581 - val_accuracy: 0.5254\n",
            "Epoch 269/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1328 - accuracy: 0.9915 - val_loss: 2.1433 - val_accuracy: 0.5424\n",
            "Epoch 270/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1331 - accuracy: 0.9873 - val_loss: 2.1377 - val_accuracy: 0.5424\n",
            "Epoch 271/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1296 - accuracy: 0.9915 - val_loss: 2.1435 - val_accuracy: 0.5424\n",
            "Epoch 272/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1314 - accuracy: 0.9915 - val_loss: 2.1370 - val_accuracy: 0.5424\n",
            "Epoch 273/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1298 - accuracy: 0.9831 - val_loss: 2.1400 - val_accuracy: 0.5424\n",
            "Epoch 274/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1274 - accuracy: 0.9915 - val_loss: 2.1507 - val_accuracy: 0.5424\n",
            "Epoch 275/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1262 - accuracy: 0.9915 - val_loss: 2.1554 - val_accuracy: 0.5424\n",
            "Epoch 276/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1253 - accuracy: 0.9873 - val_loss: 2.1481 - val_accuracy: 0.5424\n",
            "Epoch 277/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1250 - accuracy: 0.9915 - val_loss: 2.1558 - val_accuracy: 0.5424\n",
            "Epoch 278/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1238 - accuracy: 0.9915 - val_loss: 2.1577 - val_accuracy: 0.5424\n",
            "Epoch 279/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1232 - accuracy: 0.9915 - val_loss: 2.1578 - val_accuracy: 0.5424\n",
            "Epoch 280/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1216 - accuracy: 0.9915 - val_loss: 2.1591 - val_accuracy: 0.5424\n",
            "Epoch 281/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9915 - val_loss: 2.1668 - val_accuracy: 0.5424\n",
            "Epoch 282/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1199 - accuracy: 0.9873 - val_loss: 2.1774 - val_accuracy: 0.5424\n",
            "Epoch 283/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1199 - accuracy: 0.9915 - val_loss: 2.1701 - val_accuracy: 0.5424\n",
            "Epoch 284/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1183 - accuracy: 0.9915 - val_loss: 2.1759 - val_accuracy: 0.5424\n",
            "Epoch 285/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1173 - accuracy: 0.9915 - val_loss: 2.1822 - val_accuracy: 0.5424\n",
            "Epoch 286/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1164 - accuracy: 0.9915 - val_loss: 2.1917 - val_accuracy: 0.5424\n",
            "Epoch 287/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1162 - accuracy: 0.9915 - val_loss: 2.1793 - val_accuracy: 0.5424\n",
            "Epoch 288/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1159 - accuracy: 0.9915 - val_loss: 2.1763 - val_accuracy: 0.5424\n",
            "Epoch 289/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1155 - accuracy: 0.9915 - val_loss: 2.1685 - val_accuracy: 0.5424\n",
            "Epoch 290/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1133 - accuracy: 0.9915 - val_loss: 2.1797 - val_accuracy: 0.5424\n",
            "Epoch 291/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1133 - accuracy: 0.9915 - val_loss: 2.1789 - val_accuracy: 0.5424\n",
            "Epoch 292/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1121 - accuracy: 0.9915 - val_loss: 2.1853 - val_accuracy: 0.5424\n",
            "Epoch 293/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1118 - accuracy: 0.9873 - val_loss: 2.1959 - val_accuracy: 0.5424\n",
            "Epoch 294/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1106 - accuracy: 0.9873 - val_loss: 2.1894 - val_accuracy: 0.5424\n",
            "Epoch 295/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1093 - accuracy: 0.9915 - val_loss: 2.1860 - val_accuracy: 0.5424\n",
            "Epoch 296/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1091 - accuracy: 0.9873 - val_loss: 2.1839 - val_accuracy: 0.5424\n",
            "Epoch 297/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1091 - accuracy: 0.9873 - val_loss: 2.1931 - val_accuracy: 0.5424\n",
            "Epoch 298/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1071 - accuracy: 0.9915 - val_loss: 2.1978 - val_accuracy: 0.5424\n",
            "Epoch 299/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1069 - accuracy: 0.9915 - val_loss: 2.2135 - val_accuracy: 0.5424\n",
            "Epoch 300/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1061 - accuracy: 0.9915 - val_loss: 2.2186 - val_accuracy: 0.5424\n",
            "Epoch 301/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1063 - accuracy: 0.9915 - val_loss: 2.2126 - val_accuracy: 0.5424\n",
            "Epoch 302/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1050 - accuracy: 0.9873 - val_loss: 2.2192 - val_accuracy: 0.5424\n",
            "Epoch 303/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1055 - accuracy: 0.9915 - val_loss: 2.2150 - val_accuracy: 0.5424\n",
            "Epoch 304/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1045 - accuracy: 0.9873 - val_loss: 2.2178 - val_accuracy: 0.5424\n",
            "Epoch 305/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1033 - accuracy: 0.9958 - val_loss: 2.2163 - val_accuracy: 0.5424\n",
            "Epoch 306/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1016 - accuracy: 0.9915 - val_loss: 2.2414 - val_accuracy: 0.5424\n",
            "Epoch 307/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1031 - accuracy: 0.9915 - val_loss: 2.2343 - val_accuracy: 0.5424\n",
            "Epoch 308/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1027 - accuracy: 0.9915 - val_loss: 2.2378 - val_accuracy: 0.5424\n",
            "Epoch 309/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0998 - accuracy: 0.9915 - val_loss: 2.2294 - val_accuracy: 0.5424\n",
            "Epoch 310/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1012 - accuracy: 0.9915 - val_loss: 2.2222 - val_accuracy: 0.5424\n",
            "Epoch 311/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1022 - accuracy: 0.9915 - val_loss: 2.2170 - val_accuracy: 0.5424\n",
            "Epoch 312/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0993 - accuracy: 0.9915 - val_loss: 2.2033 - val_accuracy: 0.5424\n",
            "Epoch 313/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0980 - accuracy: 0.9915 - val_loss: 2.2175 - val_accuracy: 0.5424\n",
            "Epoch 314/350\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0967 - accuracy: 0.9873 - val_loss: 2.2258 - val_accuracy: 0.5424\n",
            "Epoch 315/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0961 - accuracy: 0.9873 - val_loss: 2.2294 - val_accuracy: 0.5424\n",
            "Epoch 316/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0960 - accuracy: 0.9915 - val_loss: 2.2334 - val_accuracy: 0.5424\n",
            "Epoch 317/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0951 - accuracy: 0.9873 - val_loss: 2.2348 - val_accuracy: 0.5424\n",
            "Epoch 318/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0955 - accuracy: 0.9915 - val_loss: 2.2274 - val_accuracy: 0.5424\n",
            "Epoch 319/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0942 - accuracy: 0.9915 - val_loss: 2.2304 - val_accuracy: 0.5424\n",
            "Epoch 320/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0934 - accuracy: 0.9915 - val_loss: 2.2346 - val_accuracy: 0.5424\n",
            "Epoch 321/350\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0933 - accuracy: 0.9915 - val_loss: 2.2289 - val_accuracy: 0.5424\n",
            "Epoch 322/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0921 - accuracy: 0.9915 - val_loss: 2.2362 - val_accuracy: 0.5424\n",
            "Epoch 323/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0932 - accuracy: 0.9915 - val_loss: 2.2344 - val_accuracy: 0.5424\n",
            "Epoch 324/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0922 - accuracy: 0.9915 - val_loss: 2.2360 - val_accuracy: 0.5593\n",
            "Epoch 325/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0910 - accuracy: 0.9915 - val_loss: 2.2402 - val_accuracy: 0.5424\n",
            "Epoch 326/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0897 - accuracy: 0.9915 - val_loss: 2.2364 - val_accuracy: 0.5424\n",
            "Epoch 327/350\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0897 - accuracy: 0.9873 - val_loss: 2.2437 - val_accuracy: 0.5424\n",
            "Epoch 328/350\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0888 - accuracy: 0.9915 - val_loss: 2.2463 - val_accuracy: 0.5424\n",
            "Epoch 329/350\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0896 - accuracy: 0.9915 - val_loss: 2.2450 - val_accuracy: 0.5424\n",
            "Epoch 330/350\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0913 - accuracy: 0.9873 - val_loss: 2.2457 - val_accuracy: 0.5424\n",
            "Epoch 331/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0880 - accuracy: 0.9915 - val_loss: 2.2486 - val_accuracy: 0.5424\n",
            "Epoch 332/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0884 - accuracy: 0.9915 - val_loss: 2.2655 - val_accuracy: 0.5424\n",
            "Epoch 333/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0878 - accuracy: 0.9915 - val_loss: 2.2561 - val_accuracy: 0.5424\n",
            "Epoch 334/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0864 - accuracy: 0.9915 - val_loss: 2.2622 - val_accuracy: 0.5424\n",
            "Epoch 335/350\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0873 - accuracy: 0.9873 - val_loss: 2.2563 - val_accuracy: 0.5424\n",
            "Epoch 336/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0856 - accuracy: 0.9915 - val_loss: 2.2676 - val_accuracy: 0.5424\n",
            "Epoch 337/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0848 - accuracy: 0.9915 - val_loss: 2.2692 - val_accuracy: 0.5424\n",
            "Epoch 338/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0851 - accuracy: 0.9873 - val_loss: 2.2758 - val_accuracy: 0.5593\n",
            "Epoch 339/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0840 - accuracy: 0.9873 - val_loss: 2.2788 - val_accuracy: 0.5593\n",
            "Epoch 340/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0839 - accuracy: 0.9915 - val_loss: 2.2653 - val_accuracy: 0.5593\n",
            "Epoch 341/350\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0834 - accuracy: 0.9915 - val_loss: 2.2634 - val_accuracy: 0.5593\n",
            "Epoch 342/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0825 - accuracy: 0.9915 - val_loss: 2.2727 - val_accuracy: 0.5593\n",
            "Epoch 343/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0833 - accuracy: 0.9915 - val_loss: 2.2666 - val_accuracy: 0.5593\n",
            "Epoch 344/350\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0819 - accuracy: 0.9915 - val_loss: 2.2698 - val_accuracy: 0.5593\n",
            "Epoch 345/350\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0815 - accuracy: 0.9915 - val_loss: 2.2718 - val_accuracy: 0.5593\n",
            "Epoch 346/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0828 - accuracy: 0.9915 - val_loss: 2.2845 - val_accuracy: 0.5593\n",
            "Epoch 347/350\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0875 - accuracy: 0.9915 - val_loss: 2.2775 - val_accuracy: 0.5593\n",
            "Epoch 348/350\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0826 - accuracy: 0.9915 - val_loss: 2.2885 - val_accuracy: 0.5763\n",
            "Epoch 349/350\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0803 - accuracy: 0.9958 - val_loss: 2.2844 - val_accuracy: 0.5763\n",
            "Epoch 350/350\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0804 - accuracy: 0.9915 - val_loss: 2.2933 - val_accuracy: 0.5763\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train, epochs=350, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "px82j3TwEiJB",
        "outputId": "fcbc7574-cd86-4ce2-d05b-fa3d08c6e93c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(history.history['accuracy'], label='training set accuracy')\n",
        "plt.plot(history.history['loss'], label='training set loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxf8LWWeF9wz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def predict_response(prediction_input):\n",
        "\n",
        "    words = prediction_input.split()\n",
        "    words = [word for word in words if word.lower() not in connecting_words and word.lower() not in articles]\n",
        "    prediction_input = ' '.join(words)\n",
        "    texts_p = [prediction_input]\n",
        "\n",
        "    # Tokenize and pad\n",
        "    prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
        "    prediction_input = np.array(prediction_input).reshape(-1)\n",
        "    prediction_input = pad_sequences([prediction_input], input_shape)\n",
        "\n",
        "    # Get output from model\n",
        "    output = model.predict(prediction_input)\n",
        "    output = output.argmax()\n",
        "    print(output)\n",
        "    # Finding the right tag and predicting\n",
        "    response_tag = le.inverse_transform([output])[0]\n",
        "    return random.choice(responses[response_tag])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjeJxeSqV83a",
        "outputId": "813b60d6-5d4c-4752-a847-6b2bb2ece501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.31.3-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.16.3 (from gradio)\n",
            "  Downloading gradio_client-0.16.3-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.3->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.3->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=865da5e19fa0c1fb8f8d68d83e7ce311fc2c2a46631fef73cc99c6e50124a880\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 ffmpy-0.3.2 gradio-4.31.3 gradio-client-0.16.3 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.4 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "kdDHLsy6oPXl",
        "outputId": "562c295e-bb5e-4d10-9167-82e0b5173371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://aea5711c7c09b90a8d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aea5711c7c09b90a8d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "iface = gr.Interface(fn=predict_response,inputs=\"text\",outputs=\"text\")\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred_classes, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nDpz79HswVC",
        "outputId": "fed71667-7a82-490e-bc55-5a932f1b9a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 4s 12ms/step\n",
            "Accuracy: 0.576271186440678\n",
            "Precision: 0.7001614205004035\n",
            "Recall: 0.576271186440678\n",
            "F1 Score: 0.556983019196379\n",
            "Confusion Matrix:\n",
            " [[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}